<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>An Outline of Generalized Linear Models</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>An Outline of Generalized Linear Models</h1>

<h3>2.1 Processes in Model Fitting</h3>

<p>Three steps in model fitting:</p>

<ol>
<li>Model Selection </li>
<li>Parameter Estimation </li>
<li>Prediction of Future Values</li>
</ol>

<h4>Model Selection</h4>

<p>Generalized Linear Models (GLM) assume independent, or at least uncorrelated,
observations. Can also be independent within blocks of fixed or known size.</p>

<p>Building a model means we want it to be relevant to the data under study.
Picking a scale for \(Y\) should be relavent to the data and also should, if
possible, provide </p>

<ul>
<li>Constancy of Variance</li>
<li>Approximate Normality of Errors</li>
<li>Additivity of Systematic Effects</li>
</ul>

<p>There sometimes <em>isn&#39;t</em> a scale that provides all three. </p>

<p>In GLMs we can relax the Normality and Variance assumptions as long as we
know how the variance depends on the mean. </p>

<h4>Parameter Estimation</h4>

<p>Use a goodness of fit measure to find the parameter values that maximize it.
Usually this in terms of some liklihood function. Sometimes the <em>scaled 
deviance</em> is used instead which measures how close the liklihood comes to the
maximum possible liklihood for an exact fit (you are still maximizing the
liklihood function). </p>

<p>\[
D^*(y; u) = 2l(y;y) - 2l(u;y)
\]</p>

<h4>Prediction</h4>

<p>Statements about the likely values of unobserved events. <em>Calibration</em> is 
sometimes used when when the response is fixed and we are required to make
statements about the likely values of \(x\). </p>

<p>Predictions also need to include measures of precision. </p>

<h3>2.2 Components of a GLM</h3>

<p>There are common components to all GLMs</p>

<ol>
<li>Random Component

<ul>
<li>Any exponential family of distributions works here</li>
</ul></li>
<li>Systematic Component (terms add up)</li>
<li>The Link between the two previous components

<ul>
<li>any monotonic differentiable function</li>
</ul></li>
</ol>

<p>The random component we assume independence and constant variance of errors.
These are very important.</p>

<h4>Liklihood Functions for GLMs</h4>

<p>The exponential family has the following form:</p>

<p>\[
f_{Y}(y; \theta, \phi) = \exp\{(y * \theta - b(\theta) )  /  (a(\phi) + c(y, \phi))\}
\]</p>

<p>for specific functions of \(a(.)\), \(b(.)\), and \(c(.)\). </p>

<p>To give an example for the normal distribution:</p>

<p>\[
f_{Y}(y; \theta, \phi) = \frac{1}{\sqrt{2\pi \sigma ^{2}}}\exp\{-(y - \mu)^{2}/2\sigma ^{2}\}
\]
\[
= \exp\{(y\mu - \mu ^{2}/2) / \sigma ^2 - \frac{1}{2}(y^{2}/\sigma ^{2} + log(2\pi\sigma ^{2}))\}
\]</p>

<p>So \(\theta = \mu\), \(\phi = \sigma ^2\), \(a(\phi) = \phi\), \(b(\theta) = \theta ^{2}/2\), and \(c(y, \phi) = -\frac{1}{2}(y^{2}/\sigma ^{2} + log(2\pi\sigma ^{2}))\}\)</p>

<p>We can find \(\theta\) and \(\phi\) from finding theta that maximizes the 
liklihood function by taking partial derivatives and setting equal to 0. </p>

<p>\[
E(Y) = b'(\theta)
\]</p>

<p>(which for normal distributions is just \(\mu\)) and</p>

<p>\[
var(Y) = b''(\theta)a(\phi)
\]</p>

<p>The variance consists of two functions:</p>

<ul>
<li>\(b''(\theta)\) only depends on the mean, usually called the <em>variance 
function</em> (\(V(\mu)\)). </li>
<li>\(a(\phi)\) is usually of the form \(\phi / w\). \(\phi\)
is called the dispersion parameter and is constant over observations. \(w\) is
a prior weight that varies between observations.</li>
</ul>

<h4>Link Functions</h4>

<p>The link function relates the linear predictors (\(\eta\)) to the expected
value of the response variable (\(\mu\)). The link function must map all
possible values of \(\eta\) to the domain of the response variable.</p>

<p>There are famous canonical links for different distributions.</p>

<ul>
<li>Normal:           \(\eta = \mu\)</li>
<li>Poisson:          \(\eta = \log{\mu}\)</li>
<li>Binomial:         \(\eta = \log{\pi / (1 - \pi)}\)</li>
<li>Gamma:            \(\eta = \mu^{-1}\)</li>
<li>Inverse Guassian: \(\eta = \mu^{-2}\)</li>
</ul>

<h3>2.3 Measuring The Goodness of Fit</h3>

<p>The <em>null model</em> assigns all variation to the random component of the model 
and hence predicts the same value for all $y$s. The <em>full model</em> has a 
parameter for each observation and assigns all the random variation to \(y\) 
and none to the random component. This obviously does not generalize well to 
new data.</p>

<p>The discrepency of a fit is sometimes measured as twice the difference
between the maximum log liklihood achievable and the model under
investigation, which is just the scaled deviance.</p>

<h3>2.4 Residuals</h3>

<p>residuals are whats left over after a prediction; datum = fitted value +
residual.</p>

<h4>Pearson Residual</h4>

<p>The normal residual scaled by the estimated standard deviation of \(Y\).:</p>

<p>\[ r_p = \frac{y - \mu}{\sqrt{V(\mu)}} \]</p>

<p>This can be skewed for non-normal distributions. You can use the deviance 
residuals for those</p>

<h3>2.5 An Algorithm for Fitting Generalized Linear Models</h3>

<p>The coefficients \(\beta\) can be fit using iteratively weighted least squares.
The dependent variable is now \(z\), the linearized form of the link function
that has been applied to \(Y\). </p>

<ol>
<li>Start with an initial estimate of \(\eta\) and \(\mu\).</li>
<li>find
\[
z_0 = \hat{\eta}_0 + (y - \hat{\mu}_0)\left(\frac{d\eta}{d\mu}\right)_0
\]
by evaluating the derivative at \(\hat{\mu}_0\)</li>
<li>The weights for each iteration are found by
\[
W_{0}^{-1} = \left(\frac{d\eta}{d\mu}\right)_{0}^{2} V_0
\]
where \(V_0\) is the variance function evaluated at \(\hat{\mu}_0\). </li>
<li>Regress \(z_0\) on the variables with the weights to find \(\beta_1\).</li>
<li>Repeat until changes converge. </li>
</ol>

<h3>R code for algorithms</h3>

<h4>Iteratively Reweighted Least Squares</h4>

</body>

</html>
