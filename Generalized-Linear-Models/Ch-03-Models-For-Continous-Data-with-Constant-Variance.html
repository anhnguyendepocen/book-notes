<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Models for Continous Data with Constant Variance</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Models for Continous Data with Constant Variance</h1>

<h3>3.1 Introduction</h3>

<p>This is just classic linear regression:</p>

<p>\[
Y_i \sim N(\mu_i, \sigma ^2), \mu = \eta, \eta = \sum_{1}^{p}{x_j \beta_j}
\]</p>

<p>Again these are our Random, Link, and Systemic components</p>

<h3>3.2 Error Structure</h3>

<p>The \(Y\) observations are assumed to have equal variance and be independent.
Normality isn&#39;t necessary for large samples. Constant variance is important
enough to where you basically have to check for it, but we will cover that
later. </p>

<p>If your predictions need to be restricted to positive values \(\log{Y}\) is 
often used. </p>

<h3>3.3 Systematic Component</h3>

<h4>Continuous Covariates</h4>

<p>Can use transformations as long as those transformations are linear in their 
impact. </p>

<h4>Qualitative Covariates</h4>

<p>Use dummy variables, you can&#39;t use all levels of a factor. </p>

<h3>3.5 Aliasing</h3>

<p>This deals with the possible ways to deal with including all levels of the 
factor in the model. We have three options:</p>

<ol>
<li>Make the intercept = 0 and include all levels</li>
<li>Make the first level = 0 and use that as a reference</li>
<li>Force the intercept = the group mean and the coefficients for all the 
levels are new the deviation from the group mean. This can also be weighted
to account for different group sizes. </li>
</ol>

<h4>Functional Relations Among Covariates</h4>

<p>Basically it doesn&#39;t make sense to include higher order terms without the 
lower ones (e.g. don&#39;t have \(x\) and \(x^3\) in a model without \(x^2\)). You are
just implicitly setting the coefficient of the lower terms = 0 for no reason.</p>

<h3>3.6 Estimation</h3>

<h4>The Maximum-liklihood Equations</h4>

<p>For Normal errors the log liklihood of a model is:</p>

<p>\[
-2l = n \log{(2\pi\sigma ^2)} + \sum_{i=1}^{n}(y_i - \mu_i)^{2}/\sigma ^2
\]</p>

<p>When \(\sigma ^2\) is fixed we are just minimizing the sum of square errors. 
We know </p>

<p>\[
\mu_i = \sum_{j=1}^{p}x_{ij}\beta_j
\]</p>

<p>And if we differentiate the sum of square errors with respect to \(\beta_j\) 
and set to 0 we get:</p>

<p>\[
\sum_{i}x_{ij}(y_i - \hat{\mu_i}) = 0 for j = 1,&hellip;,p
\]</p>

<p>Basically we are saying the linear combinations of $x$s and \(Y\) is equal to
the linear combination of $x$s and the fitted values, \(\mu\). Somehow this 
means that the vector of residuals, \(y_i - \hat{\mu_i}\), is orthogonal to 
the columns of the model matrix \(X\), which means:</p>

<p>\[
\mathbf{X}^T(\mathbf{y} - \mathbf{\hat{\mu}})
\]</p>

</body>

</html>
